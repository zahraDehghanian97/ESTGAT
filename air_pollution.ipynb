{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ATPfZuMoc7n",
    "papermill": {
     "duration": 0.026081,
     "end_time": "2021-12-24T18:51:03.900868",
     "exception": false,
     "start_time": "2021-12-24T18:51:03.874787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1>\n",
    "<center>Link Prediction Based Spatio-Temporal Graph Attention Networks\n",
    "for PM2.5 pollutant Forecasting</center>\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lyjnbjldoc7q",
    "papermill": {
     "duration": 0.026088,
     "end_time": "2021-12-24T18:51:03.953040",
     "exception": false,
     "start_time": "2021-12-24T18:51:03.926952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this notebook, we will dive into STGAT model where everything new meets  STGAT Attention + deep learning time series analysis on PM2.5 in Tehran air pollution dataset( spatio-temporal data) + link prediction, all in one notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "id": "yJwipdkjoc7s"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from scipy.sparse.linalg import eigs\n",
    "from tensorboardX import SummaryWriter\n",
    "import math\n",
    "from typing import Optional, List, Union\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.typing import OptTensor\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.transforms import LaplacianLambdaMax\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, get_laplacian\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_scatter import scatter_add\n",
    "from geopy import distance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from numpy.linalg import inv\n",
    "import heapq\n",
    "from geopy import distance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric_temporal.signal import StaticGraphTemporalSignal\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from six.moves import urllib\n",
    "from torch_geometric_temporal.signal import temporal_signal_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T18:51:04.017194Z",
     "iopub.status.busy": "2021-12-24T18:51:04.016348Z",
     "iopub.status.idle": "2021-12-24T18:51:06.583129Z",
     "shell.execute_reply": "2021-12-24T18:51:06.582073Z",
     "shell.execute_reply.started": "2021-12-24T18:45:02.744681Z"
    },
    "id": "uw40Qq5doc7t",
    "papermill": {
     "duration": 2.603971,
     "end_time": "2021-12-24T18:51:06.583249",
     "exception": false,
     "start_time": "2021-12-24T18:51:03.979278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num = 100\n",
    "torch.manual_seed(num)\n",
    "random.seed(num)\n",
    "np.random.seed(num)\n",
    "# USE_CUDA = torch.is_available()\n",
    "# DEVICE = torch.device('cpu')\n",
    "# print(\"CUDA:\", USE_CUDA, DEVICE)\n",
    "sw = SummaryWriter(logdir='.', flush_secs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qajSh1Xzoc7u",
    "papermill": {
     "duration": 0.027632,
     "end_time": "2021-12-24T18:51:06.638182",
     "exception": false,
     "start_time": "2021-12-24T18:51:06.610550",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading the graph adjacency matrix (Spatial part)\n",
    "to use other types of matrix generation uncomment below block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "id": "fiF5Q277oc7u"
   },
   "outputs": [],
   "source": [
    "A = np.load('weather_adj_mat.npy')\n",
    "G = nx.from_numpy_matrix(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "id": "Fy6FW68Aoc7u"
   },
   "outputs": [],
   "source": [
    "stations = pd.read_excel('station_id.xlsx', sheet_name='Sheet1')\n",
    "stations = stations.drop(labels=21, axis=0)\n",
    "stations = stations.drop(labels=15, axis=0)\n",
    "stations = stations.drop(labels=16, axis=0)\n",
    "stations = stations.reset_index(drop=True)\n",
    "station_ids = stations['id']\n",
    "distance_matrix = np.zeros((19,19))\n",
    "Latitude = stations['Latitude']\n",
    "Longitude = stations['Longitude']\n",
    "for i in range(19):\n",
    "    for j in range(19):\n",
    "        coords_1 = (Latitude[i], Longitude[i])\n",
    "        coords_2 = (Latitude[j], Longitude[j])\n",
    "        distance_matrix[i,j] = distance.geodesic(coords_1, coords_2).km "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2LZLpznoc7v"
   },
   "source": [
    "# add edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "id": "9vGMec_ooc7w"
   },
   "outputs": [],
   "source": [
    "def centrality_based(centrality_metric, graph, num_top, num_selected):\n",
    "\n",
    "    # these ones return a Dictionary of nodes with centrality as the value.\n",
    "    if centrality_metric == 'closeness':\n",
    "        centrality = nx.closeness_centrality(graph)\n",
    "    elif centrality_metric == 'degree':\n",
    "        centrality = nx.degree_centrality(graph)\n",
    "    elif centrality_metric == 'eigenvector':\n",
    "        graph = nx.DiGraph(graph)\n",
    "        centrality = nx.eigenvector_centrality(graph)\n",
    "    elif centrality_metric == 'betweenness':\n",
    "        graph = nx.DiGraph(graph)\n",
    "        centrality = nx.betweenness_centrality(graph)\n",
    "    elif centrality_metric == 'load':\n",
    "        centrality = nx.load_centrality(graph)\n",
    "    elif centrality_metric == 'subgraph':\n",
    "        graph = nx.Graph(graph)\n",
    "        centrality = nx.subgraph_centrality(graph)\n",
    "    elif centrality_metric == 'harmonic':\n",
    "        centrality = nx.harmonic_centrality(graph)\n",
    "    important_nodes = heapq.nlargest(num_top, centrality, key=centrality.get)\n",
    "    for item in important_nodes:\n",
    "        selected_nodes = np.random.choice(graph.nodes(), num_selected)\n",
    "        important_node = np.ones(len(selected_nodes))*item\n",
    "        for i in range(len(selected_nodes)):\n",
    "            for j in range(len(important_node)):\n",
    "                if distance_matrix[i,j]!=0:\n",
    "                    A[i,j] = 1/distance_matrix[i,j]\n",
    "    return A\n",
    "\n",
    "\n",
    "def link_prediction_based(pred_metric, graph, num_top):\n",
    "\n",
    "    graph = nx.Graph(graph)\n",
    "    if pred_metric == 'jaccard':\n",
    "        centrality = nx.jaccard_coefficient(graph)\n",
    "    elif pred_metric == 'common_neighbor':\n",
    "        centrality = nx.common_neighbor_centrality(graph)\n",
    "    elif pred_metric == 'preferential_attachment':\n",
    "        centrality = nx.preferential_attachment(graph)\n",
    "    elif pred_metric == 'resource_allocation':\n",
    "        centrality = nx.resource_allocation_index(graph)\n",
    "    elif pred_metric == 'adamic_adar':\n",
    "        centrality = nx.adamic_adar_index(graph)        \n",
    "    centrality = {item[:2]: item[2]  for item in centrality}\n",
    "    new_edges = heapq.nlargest(num_top, centrality, key=centrality.get)\n",
    "    new_edges = np.array(new_edges)\n",
    "    for i in range(len(new_edges)):\n",
    "        x,y = new_edges[i,0],new_edges[i,1]\n",
    "        A[x,y] = 1/distance_matrix[x,y]\n",
    "    return A\n",
    "\n",
    "A = np.load('weather_adj_mat.npy')\n",
    "G = nx.from_numpy_matrix(A)\n",
    "A = centrality_based('closeness', G, 5, 1)\n",
    "np.save('weather_adj_mat_new.npy', A)\n",
    "rows, cols = np.where(A != 1)\n",
    "edge_index_data = torch.LongTensor(np.array([rows, cols]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_N0WMsRoc7x",
    "papermill": {
     "duration": 0.027739,
     "end_time": "2021-12-24T18:51:36.430048",
     "exception": false,
     "start_time": "2021-12-24T18:51:36.402309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loading the data (Temporal part)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T18:51:36.494793Z",
     "iopub.status.busy": "2021-12-24T18:51:36.494256Z",
     "iopub.status.idle": "2021-12-24T18:51:39.470673Z",
     "shell.execute_reply": "2021-12-24T18:51:39.469670Z",
     "shell.execute_reply.started": "2021-12-24T18:45:35.569956Z"
    },
    "id": "mgu8PuXsoc7x",
    "papermill": {
     "duration": 3.013497,
     "end_time": "2021-12-24T18:51:39.470803",
     "exception": false,
     "start_time": "2021-12-24T18:51:36.457306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('weather_node_values_new.npy', 'rb') as f:\n",
    "    tweets_data_previous_hours_train = np.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hDQ3g5Yoc7x",
    "papermill": {
     "duration": 0.027459,
     "end_time": "2021-12-24T18:51:39.526220",
     "exception": false,
     "start_time": "2021-12-24T18:51:39.498761",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Creating the DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "id": "ccclS3bQoc7y"
   },
   "outputs": [],
   "source": [
    "class AirPollutionDatasetLoader(object):\n",
    "\n",
    "    def __init__(self,):\n",
    "        super(AirPollutionDatasetLoader, self).__init__()\n",
    "        # self.raw_data_dir = raw_data_dir\n",
    "        self._read_web_data()\n",
    "\n",
    "    def _download_url(self, url, save_path):  # pragma: no cover\n",
    "        with urllib.request.urlopen(url) as dl_file:\n",
    "            with open(save_path, \"wb\") as out_file:\n",
    "                out_file.write(dl_file.read())\n",
    "\n",
    "    def _read_web_data(self):\n",
    "\n",
    "\n",
    "        A = np.load(\"./weather_adj_mat_new.npy\")\n",
    "        self.A = torch.from_numpy(A)\n",
    "        X = np.load(\"./weather_node_values_new.npy\")\n",
    "        X = X.astype(np.float32)\n",
    "        temp = X[:,:,:7]\n",
    "        for i in range(7) :\n",
    "            X = temp[:,:,i]\n",
    "        # Normalise via Z-Score Method\n",
    "            means = np.mean(X)\n",
    "            print(means)\n",
    "            X = X - means\n",
    "            stds = np.std(X)\n",
    "            X = X / stds\n",
    "            temp[:,:,i] = X\n",
    "        self.X = torch.from_numpy(temp)\n",
    "        print(self.X.shape)\n",
    "\n",
    "    def _get_edges_and_weights(self):\n",
    "        edge_indices, values = dense_to_sparse(self.A)\n",
    "        edge_indices = edge_indices.numpy()\n",
    "        values = values.numpy()\n",
    "        self.edges = edge_indices\n",
    "        self.edge_weights = values\n",
    "\n",
    "    def _generate_task(self, num_timesteps_in: int = 12, num_timesteps_out: int = 1):\n",
    "       \n",
    "        indices = [\n",
    "            (i, i + (num_timesteps_in + num_timesteps_out))\n",
    "            for i in range(self.X.shape[0] - (num_timesteps_in + num_timesteps_out) + 1)\n",
    "        ]\n",
    "        features, target = [], []\n",
    "        for i, j in indices:\n",
    "            features.append((self.X[ i : i + num_timesteps_in,:,:]).numpy())\n",
    "            target.append((self.X[ i + num_timesteps_in : j,:,0]).numpy())\n",
    "        self.features = features\n",
    "        self.targets = target\n",
    "\n",
    "    def get_dataset(\n",
    "        self, num_timesteps_in: int = 12, num_timesteps_out: int = 1\n",
    "    ) -> StaticGraphTemporalSignal:\n",
    "        self._get_edges_and_weights()\n",
    "        self._generate_task(num_timesteps_in, num_timesteps_out)\n",
    "        dataset = StaticGraphTemporalSignal(\n",
    "            self.edges, self.edge_weights, self.features, self.targets\n",
    "        )\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "id": "e2WxYIFWoc7y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.0058\n",
      "9.153586\n",
      "1016.07367\n",
      "0.27147993\n",
      "37.53304\n",
      "18.185081\n",
      "202.95992\n",
      "torch.Size([2543, 19, 7])\n"
     ]
    }
   ],
   "source": [
    "loader = AirPollutionDatasetLoader()\n",
    "dataset = loader.get_dataset(num_timesteps_in = 12, num_timesteps_out= 1)\n",
    "dataset_ = np.array(list(dataset.features))\n",
    "a, b, c , d = np.array(list(dataset.features)).shape\n",
    "dataset_ = dataset_.reshape(a,c, d, b)\n",
    "dataset_.shape\n",
    "dataset.features = dataset_\n",
    "dataset_ = np.array(list(dataset.targets))\n",
    "a, b, c = np.array(list(dataset.targets)).shape\n",
    "dataset_ = dataset_.reshape(a,c,b)\n",
    "dataset_.shape\n",
    "dataset.targets = dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t1CLD28Woc7z",
    "outputId": "8d5863d7-9b82-4781-d82f-7c7726d10cde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: torch.Size([506, 19, 7, 12]) torch.Size([506, 19, 1])\n",
      "test: torch.Size([2025, 19, 7, 12]) torch.Size([2025, 19, 1])\n"
     ]
    }
   ],
   "source": [
    "shuffle=True\n",
    "batch_size = 16\n",
    "train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n",
    "\n",
    "train_x_tensor = torch.from_numpy(np.array(train_dataset.features)).type(torch.FloatTensor)  # (B, N, F, T)\n",
    "train_target_tensor = torch.from_numpy(np.array(train_dataset.targets)).type(torch.FloatTensor)  # (B, N, T)\n",
    "train_dataset = torch.utils.data.TensorDataset(train_x_tensor, train_target_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "test_x_tensor = torch.from_numpy(np.array(test_dataset.features)).type(torch.FloatTensor)  # (B, N, F, T)\n",
    "test_target_tensor = torch.from_numpy(np.array(test_dataset.targets)).type(torch.FloatTensor)  # (B, N, T)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_x_tensor, test_target_tensor)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "print('train:', train_x_tensor.size(), train_target_tensor.size())\n",
    "print('test:', test_x_tensor.size(), test_target_tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vJPORNOoc7z",
    "papermill": {
     "duration": 0.027332,
     "end_time": "2021-12-24T18:51:39.688841",
     "exception": false,
     "start_time": "2021-12-24T18:51:39.661509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Defining Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal attention layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T18:51:39.871257Z",
     "iopub.status.busy": "2021-12-24T18:51:39.870009Z",
     "iopub.status.idle": "2021-12-24T18:51:39.873064Z",
     "shell.execute_reply": "2021-12-24T18:51:39.872647Z",
     "shell.execute_reply.started": "2021-12-24T18:45:39.177412Z"
    },
    "id": "nPXBwYoLoc70",
    "papermill": {
     "duration": 0.044347,
     "end_time": "2021-12-24T18:51:39.873162",
     "exception": false,
     "start_time": "2021-12-24T18:51:39.828815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TemporalAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels: int, num_of_vertices: int, num_of_timesteps: int, DEVICE = torch.device('cpu')):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "\n",
    "        self._U1 = nn.Parameter(torch.FloatTensor(num_of_vertices))  # N\n",
    "        self._U2 = nn.Parameter(torch.FloatTensor(in_channels)) # F\n",
    "        self._U3 = nn.Parameter(torch.FloatTensor(num_of_timesteps)) # T\n",
    "        self._U4 = nn.Parameter(torch.FloatTensor(num_of_timesteps)) # T\n",
    "        self._reset_parameters()\n",
    "        self._LHS = torch.FloatTensor(num_of_timesteps) # T\n",
    "        self._RHS = torch.FloatTensor(num_of_timesteps) # T\n",
    "        \n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "            else:\n",
    "                nn.init.uniform_(p)\n",
    "                \n",
    "    def _generate_LHS_RHS(self,X):\n",
    "        self._LHS = torch.mul(X,self._U3)\n",
    "        self._RHS = torch.mul(X,self._U4)  \n",
    "        \n",
    "    def _aggregate(self,type_ensemble = 'sum') :\n",
    "        size=np.array(self._LHS.size()).tolist()\n",
    "        size.append(size[1])\n",
    "        e = torch.FloatTensor(size[0],size[1],size[2])\n",
    "        if type_ensemble == 'sum' :\n",
    "            for b in range(size[0]): # b\n",
    "                for i in range(size[1]): #N\n",
    "                    for j in range (size[2]) : #N\n",
    "                        e[b,i,j] = self._LHS[b,i] + self._RHS[b,j]\n",
    "        return e\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        x = torch.permute(X,(0,3,2,1)) # (b,N,F,T) -> (b,T,F,N)\n",
    "        x = torch.matmul(torch.matmul(x, self._U1),self._U2) # (b,T,F,N)(N)->(b,T,F)-> (b,T)\n",
    "        self._generate_LHS_RHS(x) #(b,T) , (b,T)\n",
    "        type_ensemble = 'sum'\n",
    "        e = self._aggregate(type_ensemble) #(b,T,T)\n",
    "        e = F.softmax(torch.sigmoid(e),dim = 1) #(b,T,T)\n",
    "        return e\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDH_ihC4oc70",
    "papermill": {
     "duration": 0.029586,
     "end_time": "2021-12-24T18:51:39.930891",
     "exception": false,
     "start_time": "2021-12-24T18:51:39.901305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Spatial attention layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T18:51:40.064312Z",
     "iopub.status.busy": "2021-12-24T18:51:40.063549Z",
     "iopub.status.idle": "2021-12-24T18:51:40.066308Z",
     "shell.execute_reply": "2021-12-24T18:51:40.065891Z",
     "shell.execute_reply.started": "2021-12-24T18:45:39.193937Z"
    },
    "id": "oN3MTc1Doc70",
    "papermill": {
     "duration": 0.043302,
     "end_time": "2021-12-24T18:51:40.066398",
     "exception": false,
     "start_time": "2021-12-24T18:51:40.023096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, in_channels: int, num_of_vertices: int, num_of_timesteps: int, DEVICE = torch.device('cpu')):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self._W1 = nn.Parameter(torch.FloatTensor(num_of_timesteps))  # T\n",
    "        self._W2 = nn.Parameter(torch.FloatTensor(in_channels))  # F\n",
    "        self._W3 = nn.Parameter(torch.FloatTensor(num_of_vertices))   # N\n",
    "        self._W4 = nn.Parameter(torch.FloatTensor(num_of_vertices))   # N\n",
    "        self._reset_parameters()\n",
    "        self._LHS = torch.FloatTensor( num_of_vertices) #N\n",
    "        self._RHS = torch.FloatTensor( num_of_vertices) #N\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "            else:\n",
    "                nn.init.uniform_(p)\n",
    "    \n",
    "    def _generate_LHS_RHS(self,X):\n",
    "        self._LHS = torch.mul(X,self._W3)\n",
    "        self._RHS = torch.mul(X,self._W4)  \n",
    "        \n",
    "    def _aggregate(self,type_ensemble = 'sum') :\n",
    "        size=np.array(self._LHS.size()).tolist()\n",
    "        size.append(size[1])\n",
    "        s = torch.FloatTensor(size[0],size[1],size[2])\n",
    "        if type_ensemble == 'sum' :\n",
    "            for b in range(size[0]): # b\n",
    "                for i in range(size[1]): #N\n",
    "                    for j in range (size[2]) : #N\n",
    "                        s[b,i,j] = self._LHS[b,i] + self._RHS[b,j]\n",
    "        return s\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        X = torch.matmul(torch.matmul(x, self._W1),self._W2) # (b,N,F,T)(T)->(b,N,F)-> (b,N)\n",
    "        self._generate_LHS_RHS(X) #(b,N) , (b,N)\n",
    "        type_ensemble = 'sum'\n",
    "        s = self._aggregate(type_ensemble) #(b,N,N)\n",
    "        s = F.softmax(torch.sigmoid(s),dim = 1) #(b,N,N)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cB8E_uU-oc71",
    "papermill": {
     "duration": 0.027132,
     "end_time": "2021-12-24T18:51:40.121364",
     "exception": false,
     "start_time": "2021-12-24T18:51:40.094232",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9kRagyKoc71",
    "papermill": {
     "duration": 0.047778,
     "end_time": "2021-12-24T18:51:40.277112",
     "exception": false,
     "start_time": "2021-12-24T18:51:40.229334",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Compute graph Laplacian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T18:51:40.401073Z",
     "iopub.status.busy": "2021-12-24T18:51:40.400156Z",
     "iopub.status.idle": "2021-12-24T18:51:40.437221Z",
     "shell.execute_reply": "2021-12-24T18:51:40.438229Z",
     "shell.execute_reply.started": "2021-12-24T18:45:39.211274Z"
    },
    "id": "ObkeGmZBoc71",
    "papermill": {
     "duration": 0.112536,
     "end_time": "2021-12-24T18:51:40.438387",
     "exception": false,
     "start_time": "2021-12-24T18:51:40.325851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ChebConvAttention(MessagePassing):\n",
    "    \n",
    "    def __init__(self, in_channels: int,out_channels: int,K: int, normalization: Optional[str] = None, bias: bool = True,**kwargs):\n",
    "        kwargs.setdefault(\"aggr\", \"add\")\n",
    "        super(ChebConvAttention, self).__init__(**kwargs)\n",
    "        assert K > 0\n",
    "        assert normalization in [None, \"sym\", \"rw\"], \"Invalid normalization\"\n",
    "\n",
    "        self._in_channels = in_channels\n",
    "        self._out_channels = out_channels\n",
    "        self._normalization = normalization\n",
    "        self._weight = Parameter(torch.Tensor(K, in_channels, out_channels))  #    self.Theta  [3, 1, 64]\n",
    "        if bias:\n",
    "            self._bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter(\"_bias\", None)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self._weight)\n",
    "        if self._bias is not None:\n",
    "            nn.init.uniform_(self._bias)\n",
    "            \n",
    "    def __norm__( self, edge_index, num_nodes: Optional[int], edge_weight: OptTensor,normalization: Optional[str],  lambda_max, \n",
    "                 dtype: Optional[int] = None, batch: OptTensor = None ):\n",
    "        edge_index, edge_weight = remove_self_loops(edge_index, edge_weight)\n",
    "        edge_index, edge_weight = get_laplacian(edge_index, edge_weight, normalization, dtype, num_nodes)\n",
    "        if batch is not None and lambda_max.numel() > 1:\n",
    "            lambda_max = lambda_max[batch[edge_index[0]]]\n",
    "        edge_weight = (2.0 * edge_weight) / lambda_max\n",
    "        edge_weight.masked_fill_(edge_weight == float(\"inf\"), 0)\n",
    "        assert edge_weight is not None\n",
    "        return edge_index, edge_weight # 307 nodes as deg, 340 edges , 307 nodes as self connections\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.FloatTensor, edge_index: torch.LongTensor, spatial_attention: torch.FloatTensor, \n",
    "                edge_weight: OptTensor = None, batch: OptTensor = None, lambda_max: OptTensor = None,) -> torch.FloatTensor:\n",
    "        if self._normalization != \"sym\" and lambda_max is None:\n",
    "            raise ValueError( \"You need to pass `lambda_max` to `forward() in case the normalization is non-symmetric.\")\n",
    "        if lambda_max is None:\n",
    "            lambda_max = torch.tensor(2.0, dtype=x.dtype, device=x.device)\n",
    "        if not isinstance(lambda_max, torch.Tensor):\n",
    "            lambda_max = torch.tensor(lambda_max, dtype=x.dtype, device=x.device)\n",
    "        assert lambda_max is not None\n",
    "        edge_index, norm = self.__norm__(edge_index, x.size(self.node_dim), edge_weight, self._normalization, lambda_max, \n",
    "                                         dtype=x.dtype, batch=batch)\n",
    "        row, col = edge_index # refer to the index of each note each is a list of nodes not a number # (954, 954)   \n",
    "        Att_norm = norm * spatial_attention[:, row, col] # spatial_attention (32, 307, 307), -> (954) * (32, 954) -> (32, 954)\n",
    "        num_nodes = x.size(self.node_dim) # 307\n",
    "        # (307, 307) * (32, 307, 307) -> (32, 307, 307) -permute-> (32, 307,307) * (32, 307, 1) -> (32, 307, 1)\n",
    "        TAx_0 = torch.matmul((torch.eye(num_nodes).to(edge_index.device) * spatial_attention).permute( 0, 2, 1), x) # (32, 307, 1)\n",
    "        out = torch.matmul(TAx_0, self._weight[0]) # (32, 307, 1) * [1, 64] -> (32, 307, 64)\n",
    "        edge_index_transpose = edge_index[[1, 0]]\n",
    "        if self._weight.size(0) > 1: # Do once\n",
    "            TAx_1 = self.propagate( edge_index_transpose, x=TAx_0, norm=Att_norm, size=None)\n",
    "            out = out + torch.matmul(TAx_1, self._weight[1])\n",
    "        for k in range(2, self._weight.size(0)): # Do once\n",
    "            TAx_2 = self.propagate(edge_index_transpose, x=TAx_1, norm=norm, size=None)\n",
    "            TAx_2 = 2.0 * TAx_2 - TAx_0\n",
    "            out = out + torch.matmul(TAx_2, self._weight[k])\n",
    "            TAx_0, TAx_1 = TAx_1, TAx_2\n",
    "        if self._bias is not None:\n",
    "            out += self._bias\n",
    "        return out #? (b, N, F_out) (32, 307, 64)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        if norm.dim() == 1: # do this\n",
    "            return norm.view(-1, 1) * x_j  # (954, 1) * (32, 954, 1) -> (32, 954, 1)\n",
    "        else:\n",
    "            d1, d2 = norm.shape\n",
    "            return norm.view(d1, d2, 1) * x_j\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}({}, {}, K={}, normalization={})\".format(self.__class__.__name__,self._in_channels,self._out_channels,self._weight.size(0),self._normalization,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAs2OhHCoc72",
    "papermill": {
     "duration": 0.045347,
     "end_time": "2021-12-24T18:51:40.530334",
     "exception": false,
     "start_time": "2021-12-24T18:51:40.484987",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The ASTGCN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T18:51:40.633782Z",
     "iopub.status.busy": "2021-12-24T18:51:40.632918Z",
     "iopub.status.idle": "2021-12-24T18:51:40.662284Z",
     "shell.execute_reply": "2021-12-24T18:51:40.663293Z",
     "shell.execute_reply.started": "2021-12-24T18:45:39.246229Z"
    },
    "id": "pU4A0OyEoc72",
    "papermill": {
     "duration": 0.087414,
     "end_time": "2021-12-24T18:51:40.663456",
     "exception": false,
     "start_time": "2021-12-24T18:51:40.576042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ASTGCNBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels: int,K: int,nb_chev_filter: int,nb_time_filter: int,time_strides: int,num_of_vertices: int,\n",
    "        num_of_timesteps: int,normalization: Optional[str] = None,bias: bool = True, DEVICE = torch.device('cpu')):\n",
    "        super(ASTGCNBlock, self).__init__()\n",
    "        self._temporal_attention = TemporalAttention(in_channels, num_of_vertices, num_of_timesteps, DEVICE)\n",
    "        self._spatial_attention = SpatialAttention(in_channels, num_of_vertices, num_of_timesteps, DEVICE)\n",
    "        self._chebconv_attention = ChebConvAttention(in_channels, nb_chev_filter, K, normalization, bias)\n",
    "        self._time_convolution = nn.Conv2d( nb_chev_filter,nb_time_filter,kernel_size=(1, 3), stride=(1, time_strides),padding=(0, 1))\n",
    "        self._residual_convolution = nn.Conv2d(in_channels, nb_time_filter, kernel_size=(1, 1), stride=(1, time_strides))\n",
    "        self._layer_norm = nn.LayerNorm(nb_time_filter) #channel\n",
    "        self._normalization = normalization\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "            else:\n",
    "                nn.init.uniform_(p)\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor, edge_index: Union[torch.LongTensor, List[torch.LongTensor]]) -> torch.FloatTensor:\n",
    "        batch_size, num_of_vertices, num_of_features, num_of_timesteps = X.shape  # (32, 307, 1, 12)\n",
    "        X_tilde = self._temporal_attention(X) # (b, T, T)  (32, 12, 12) * reshaped x(32, 307, 12)  -reshape> (32, 307, 1, 12)\n",
    "        X_tilde = torch.matmul(X.reshape(batch_size, -1, num_of_timesteps), X_tilde)\n",
    "        X_tilde = X_tilde.reshape(batch_size, num_of_vertices, num_of_features, num_of_timesteps)\n",
    "        X_tilde = self._spatial_attention(X_tilde) # (32, 307, 307)\n",
    "        if not isinstance(edge_index, list):\n",
    "            data = Data(edge_index=edge_index, edge_attr=None, num_nodes=num_of_vertices)\n",
    "            if self._normalization != \"sym\":\n",
    "                lambda_max = LaplacianLambdaMax()(data).lambda_max\n",
    "            else:\n",
    "                lambda_max = None\n",
    "            X_hat = []\n",
    "            for t in range(num_of_timesteps):\n",
    "                X_hat.append(torch.unsqueeze( self._chebconv_attention(X[:, :, :, t], edge_index, X_tilde, lambda_max=lambda_max), -1))\n",
    "            X_hat = F.relu(torch.cat(X_hat, dim=-1))\n",
    "        else:\n",
    "            X_hat = []\n",
    "            for t in range(num_of_timesteps):\n",
    "                data = Data(edge_index=edge_index[t], edge_attr=None, num_nodes=num_of_vertices)\n",
    "                if self._normalization != \"sym\":\n",
    "                    lambda_max = LaplacianLambdaMax()(data).lambda_max\n",
    "                else:\n",
    "                    lambda_max = None\n",
    "                X_hat.append(torch.unsqueeze(self._chebconv_attention(X[:, :, :, t], edge_index[t], X_tilde, lambda_max=lambda_max),-1,))\n",
    "            X_hat = F.relu(torch.cat(X_hat, dim=-1))\n",
    "\n",
    "        X_hat = self._time_convolution(X_hat.permute(0, 2, 1, 3))   # (b,N,F,T)->(b,F,N,T) (1,3)->(b,F,N,T) (32, 64, 307, 12)\n",
    "        X = self._residual_convolution(X.permute(0, 2, 1, 3)) # (b,N,F,T)->(b,F,N,T) (1,1)->(b,F,N,T)  (32, 64, 307, 12)\n",
    "        X = self._layer_norm(F.relu(X + X_hat).permute(0, 3, 2, 1))\n",
    "        X = X.permute(0, 2, 3, 1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uW495Fsoc73",
    "papermill": {
     "duration": 0.039954,
     "end_time": "2021-12-24T18:51:40.749814",
     "exception": false,
     "start_time": "2021-12-24T18:51:40.709860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# model structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T18:51:40.822691Z",
     "iopub.status.busy": "2021-12-24T18:51:40.820806Z",
     "iopub.status.idle": "2021-12-24T18:51:40.823263Z",
     "shell.execute_reply": "2021-12-24T18:51:40.823713Z",
     "shell.execute_reply.started": "2021-12-24T18:45:39.277998Z"
    },
    "id": "OdzoN84foc73",
    "papermill": {
     "duration": 0.046564,
     "end_time": "2021-12-24T18:51:40.823836",
     "exception": false,
     "start_time": "2021-12-24T18:51:40.777272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ASTGCN(nn.Module):\n",
    "    \n",
    "    def __init__( self,  nb_block: int, in_channels: int,  K: int, nb_chev_filter: int,  nb_time_filter: int,  time_strides: int,\n",
    "        num_for_predict: int,len_input: int, num_of_vertices: int, normalization: Optional[str] = None,bias: bool = True, DEVICE = torch.device('cpu')):\n",
    "        super(ASTGCN, self).__init__()\n",
    "        self._blocklist = nn.ModuleList([ASTGCNBlock( in_channels, K, nb_chev_filter, nb_time_filter,time_strides, num_of_vertices, \n",
    "                                                     len_input, normalization,bias, DEVICE)])\n",
    "        self._blocklist.extend( [ASTGCNBlock( nb_time_filter,  K, nb_chev_filter, nb_time_filter, 1, num_of_vertices, \n",
    "                                             len_input // time_strides, normalization,  bias, DEVICE)  for _ in range(nb_block - 1) ]) # nb_block= 2\n",
    "        self._final_conv = nn.Conv2d( int(len_input / time_strides),   num_for_predict,  kernel_size=(1, nb_time_filter))\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "            else:\n",
    "                nn.init.uniform_(p)\n",
    "                \n",
    "    def forward(  self, X: torch.FloatTensor, edge_index: torch.LongTensor ) -> torch.FloatTensor:\n",
    "        for block in self._blocklist:\n",
    "            X = block(X, edge_index) \n",
    "        X = self._final_conv(X.permute(0, 3, 1, 2))\n",
    "        X = X[:, :, :, -1]\n",
    "        X = X.permute(0, 2, 1) \n",
    "        return X "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7X3bFsuRoc73",
    "papermill": {
     "duration": 0.027417,
     "end_time": "2021-12-24T18:51:40.878689",
     "exception": false,
     "start_time": "2021-12-24T18:51:40.851272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T18:51:40.940969Z",
     "iopub.status.busy": "2021-12-24T18:51:40.940335Z",
     "iopub.status.idle": "2021-12-24T18:51:40.971390Z",
     "shell.execute_reply": "2021-12-24T18:51:40.971878Z",
     "shell.execute_reply.started": "2021-12-24T18:45:39.297138Z"
    },
    "id": "iW9waYhpoc73",
    "papermill": {
     "duration": 0.065597,
     "end_time": "2021-12-24T18:51:40.972002",
     "exception": false,
     "start_time": "2021-12-24T18:51:40.906405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nb_block = 2\n",
    "in_channels = 7\n",
    "K = 3\n",
    "nb_chev_filter = 64\n",
    "nb_time_filter = 64\n",
    "time_strides = 1 #num_of_hours\n",
    "num_for_predict = 1\n",
    "len_input = 12\n",
    "num_of_vertices = 19\n",
    "learning_rate = 0.0001\n",
    "\n",
    "def masked_mae(preds, labels, null_val=np.nan):\n",
    "    if np.isnan(null_val):\n",
    "        mask = ~torch.isnan(labels)\n",
    "    else:\n",
    "        mask = (labels != null_val)\n",
    "    mask = mask.float()\n",
    "    mask /= torch.mean((mask))\n",
    "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
    "    loss = torch.abs(preds - labels)\n",
    "    loss = loss * mask\n",
    "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "net = ASTGCN( nb_block, in_channels, K, nb_chev_filter, nb_time_filter, time_strides, num_for_predict, len_input, num_of_vertices)\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "masked_flag=0\n",
    "criterion = nn.L1Loss()\n",
    "criterion_masked = masked_mae\n",
    "loss_function = 'mse'\n",
    "metric_method = 'unmask'\n",
    "missing_value=0.0\n",
    "loss_function = 'rmse'\n",
    "if loss_function=='masked_mse':\n",
    "    criterion_masked = masked_mse         #nn.MSELoss().to(DEVICE)\n",
    "    masked_flag=1\n",
    "elif loss_function=='masked_mae':\n",
    "    criterion_masked = masked_mae\n",
    "    masked_flag = 1\n",
    "elif loss_function == 'mae':\n",
    "    criterion = nn.L1Loss()\n",
    "    masked_flag = 0\n",
    "elif loss_function == 'rmse':\n",
    "    criterion = nn.MSELoss()\n",
    "    masked_flag= 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T18:51:41.477906Z",
     "iopub.status.busy": "2021-12-24T18:51:41.477210Z",
     "iopub.status.idle": "2021-12-24T18:51:41.480457Z",
     "shell.execute_reply": "2021-12-24T18:51:41.480034Z",
     "shell.execute_reply.started": "2021-12-24T18:45:39.424184Z"
    },
    "id": "4vhuyFj0oc74",
    "papermill": {
     "duration": 0.056507,
     "end_time": "2021-12-24T18:51:41.480542",
     "exception": false,
     "start_time": "2021-12-24T18:51:41.424035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_val_loss_mstgcn(net, val_loader, criterion,  masked_flag,missing_value,sw, epoch, edge_index_data, limit=None):\n",
    "    net.train(False)  # ensure dropout layers are in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        val_loader_length = len(val_loader)  # nb of batch\n",
    "        tmp = []  # batch loss\n",
    "        for batch_index, batch_data in enumerate(val_loader):\n",
    "            encoder_inputs, labels = batch_data\n",
    "            outputs = net(encoder_inputs, edge_index_data)\n",
    "            if masked_flag:\n",
    "                loss = criterion(outputs, labels, missing_value)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            tmp.append(loss.item())\n",
    "            if batch_index % 100 == 0:\n",
    "                print('validation batch %s / %s, loss: %.2f' % (batch_index + 1, val_loader_length, loss.item()))\n",
    "            if (limit is not None) and batch_index >= limit:\n",
    "                break\n",
    "\n",
    "        validation_loss = sum(tmp) / len(tmp)\n",
    "        sw.add_scalar('validation_loss', validation_loss, epoch)\n",
    "    return validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T18:51:41.541704Z",
     "iopub.status.busy": "2021-12-24T18:51:41.540922Z",
     "iopub.status.idle": "2021-12-24T18:51:41.543528Z",
     "shell.execute_reply": "2021-12-24T18:51:41.543945Z",
     "shell.execute_reply.started": "2021-12-24T18:45:39.438434Z"
    },
    "id": "2iiLMS3noc74",
    "papermill": {
     "duration": 0.035195,
     "end_time": "2021-12-24T18:51:41.544053",
     "exception": false,
     "start_time": "2021-12-24T18:51:41.508858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "best_epoch = 0\n",
    "best_val_loss = np.inf\n",
    "start_time= time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-24T18:51:41.604318Z",
     "iopub.status.busy": "2021-12-24T18:51:41.603816Z",
     "iopub.status.idle": "2021-12-24T18:51:41.608536Z",
     "shell.execute_reply": "2021-12-24T18:51:41.608103Z",
     "shell.execute_reply.started": "2021-12-24T18:45:39.447350Z"
    },
    "id": "V_Fwauxnoc75",
    "outputId": "cfccc780-6dd5-469b-83f9-10179a083862",
    "papermill": {
     "duration": 0.036362,
     "end_time": "2021-12-24T18:51:41.608657",
     "exception": false,
     "start_time": "2021-12-24T18:51:41.572295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "execution": {
     "iopub.execute_input": "2021-12-24T18:51:41.682436Z",
     "iopub.status.busy": "2021-12-24T18:51:41.679421Z",
     "iopub.status.idle": "2021-12-24T20:04:13.982562Z",
     "shell.execute_reply": "2021-12-24T20:04:13.980288Z",
     "shell.execute_reply.started": "2021-12-24T18:45:39.457187Z"
    },
    "id": "TazOJwU_oc75",
    "outputId": "eccaab25-60fa-45c7-e953-6b8e138bcca3",
    "papermill": {
     "duration": 4352.345504,
     "end_time": "2021-12-24T20:04:13.982746",
     "exception": false,
     "start_time": "2021-12-24T18:51:41.637242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation batch 1 / 32, loss: 2.70\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "for epoch in range(10):\n",
    "    params_filename = os.path.join('./', 'epoch_%s.params' % epoch)\n",
    "    masked_flag = 1\n",
    "    if masked_flag:\n",
    "        val_loss = compute_val_loss_mstgcn(net, train_loader, criterion_masked, masked_flag,missing_value,sw, epoch,edge_index_data)\n",
    "    else:\n",
    "        val_loss = compute_val_loss_mstgcn(net, train_loader, criterion, masked_flag, missing_value, sw, epoch,edge_index_data)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "#         torch.save(net.state_dict(), params_filename)\n",
    "#         print('save parameters to file: %s' % params_filename)\n",
    "\n",
    "    net.train()  \n",
    "    for _ in range(10):\n",
    "        for batch_index, batch_data in enumerate(train_loader):\n",
    "            encoder_inputs, labels = batch_data   # encoder_inputs torch.Size([32, 307, 1, 12])  label torch.Size([32, 307, 12])\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(encoder_inputs, edge_index_data) # torch.Size([32, 307, 12])\n",
    "\n",
    "            if masked_flag:\n",
    "                loss = criterion_masked(outputs, labels,missing_value)\n",
    "            else :\n",
    "                loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss = loss.item()\n",
    "            global_step += 1\n",
    "            sw.add_scalar('training_loss', training_loss, global_step)\n",
    "\n",
    "            if global_step % 100 == 0:\n",
    "                print('global step: %s, training loss: %.2f, time: %.2fs' % (global_step, training_loss, time() - start_time))\n",
    "#             if global_step % 200 == 0:\n",
    "#                 #plt.plot(np.concatenate([encoder_inputs[0][0][0].cpu().numpy(), labels[0][0].cpu().numpy()]))\n",
    "#                 plt.plot(range(12),encoder_inputs[0][0][0].cpu().numpy(), color = 'red')\n",
    "#                 plt.plot(range(12,96), labels[0][0].cpu().numpy(), color='blue')\n",
    "#                 plt.show()\n",
    "#                 plt.plot(range(12),encoder_inputs[0][0][0].cpu().numpy(), color = 'red')\n",
    "#                 plt.plot(range(12,96), outputs[0][0].detach().cpu().numpy(), color='blue')\n",
    "#                 plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  s = torch.Tensor.new_zeros(size = (16,19,19),dtype=torch.float64)\n",
    "tensor = torch.zeros((16,19,19))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4dooTgUoc75",
    "papermill": {
     "duration": 0.308181,
     "end_time": "2021-12-24T20:04:14.561113",
     "exception": false,
     "start_time": "2021-12-24T20:04:14.252932",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T20:04:15.093778Z",
     "iopub.status.busy": "2021-12-24T20:04:15.092908Z",
     "iopub.status.idle": "2021-12-24T20:04:17.328395Z",
     "shell.execute_reply": "2021-12-24T20:04:17.328822Z",
     "shell.execute_reply.started": "2021-12-24T18:49:48.089181Z"
    },
    "id": "vSxPPbDWoc76",
    "papermill": {
     "duration": 2.446861,
     "end_time": "2021-12-24T20:04:17.328960",
     "exception": false,
     "start_time": "2021-12-24T20:04:14.882099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "net.train(False) \n",
    "output = []\n",
    "label = []\n",
    "with torch.no_grad():\n",
    "    test_loader_length = len(test_loader)  # nb of batch\n",
    "    tmp = []  # batch loss\n",
    "    for batch_index, batch_data in enumerate(test_loader):\n",
    "        encoder_inputs, labels = batch_data\n",
    "        outputs = net(encoder_inputs, edge_index_data)\n",
    "        for i in range(len(outputs)):\n",
    "            output.append(outputs[i])\n",
    "            label.append(labels[i])\n",
    "        loss = criterion(outputs, labels)\n",
    "        tmp.append(loss.item())\n",
    "        if batch_index % 100 == 0:\n",
    "            print('test_loss batch %s / %s, loss: %.2f' % (batch_index + 1, test_loader_length, loss.item()))\n",
    "\n",
    "    test_loss = sum(tmp) / len(tmp)\n",
    "    sw.add_scalar('test_loss', test_loss, epoch)\n",
    "print(test_loss)\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCJRnztRoc76"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sensor = 1\n",
    "timestep = 0\n",
    "preds = np.asarray([pred[sensor][timestep].detach().cpu().numpy() for pred in output])\n",
    "print(preds)\n",
    "labs  = np.asarray([l[sensor][timestep].cpu().numpy() for l in label])\n",
    "print(\"Data points:,\", preds.shape)\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(20,5))\n",
    "sns.lineplot(data=labs, label=\"true\")\n",
    "sns.lineplot(data=preds, label=\"pred\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "astgcn-for-air-pollution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "papermill": {
   "duration": 4433.693264,
   "end_time": "2021-12-24T20:04:18.722587",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-24T18:50:25.029323",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
